# -*- coding: utf-8 -*-
"""Lab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15qFOPUdpDHh0Ot1AJ2YRO9Af4YLYEEAy
"""

import pandas as pd

import numpy as np

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score, make_scorer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

import warnings

warnings.filterwarnings("ignore")

test_data = pd.read_csv("TestData_new.csv")
train_data = pd.read_csv("TrainData_new.csv")

test_data_missing = test_data.isnull().sum()
train_data_missing = train_data.isnull().sum()
train_data_missing_result = train_data_missing[train_data_missing > 0].count()

print("TraingData Missing:\n", train_data_missing)

print(f"Count of cols with missing data in TrainData: {train_data_missing_result}")

print("TestData Missing:\n", test_data_missing)

col_with_most_missings = train_data_missing.idxmax()

mean = train_data[col_with_most_missings].mean()
train_data[col_with_most_missings].fillna(mean, inplace=True)
rounded_mean = round(mean, 1)

print(f"Mean for missings: {rounded_mean}")

col_with_less_missings = train_data_missing[train_data_missing > 0].idxmin()

rows_with_less_missings = train_data[train_data[col_with_less_missings].isnull()]
count_of_rows = len(rows_with_less_missings)
train_data.dropna(subset=[col_with_less_missings], inplace=True)

print(f"Number of deleted string: {count_of_rows}")

cols_with_few_unique_values = [col for col in train_data.columns if train_data[col].nunique() < 5 and col != "target"]
print(f"Number of cols with less than 5 unique values: {len(cols_with_few_unique_values)}")

mean_feature_2 = train_data["feature_2"].mean()
median_feature_13 = train_data["feature_13"].median()

condition = (train_data['feature_2'] > mean_feature_2) & (train_data['feature_13'] < median_feature_13)
left_customers = train_data[condition]
left_ratio = round(left_customers["target"].mean(), 2)

print(f"Ratio of left: {left_ratio}")

X = train_data.drop(columns=['target'])
y = train_data['target']

print(train_data.dtypes)

numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

numeric_transformer = SimpleImputer(strategy='mean')

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(random_state=42, max_iter=1000, solver='saga'))
])

f1_scorer = make_scorer(f1_score)
f1_scores = cross_val_score(model, X, y, cv=3, scoring=f1_scorer)
mean_f1_score = round(f1_scores.mean(), 2)

print(f"Mean f1-score: {mean_f1_score}")

param_grid = {'classifier__C': np.logspace(-3, 2, num=6)}
grid_search = GridSearchCV(model, param_grid, cv=3, scoring=f1_scorer)
grid_search.fit(X, y)
best_C = grid_search.best_params_['classifier__C']

print(f"Best C value: {best_C}")

train_data['NEW'] = train_data['feature_7'] * train_data['feature_11']

X_train = train_data.drop(columns=['target'])
y_train = train_data['target']
numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

numeric_transformer = SimpleImputer(strategy='mean')
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(random_state=42))
])

param_grid = {'classifier__C': np.logspace(-3, 2, num=6)}

f1_scorer = make_scorer(f1_score)
grid_search = GridSearchCV(model, param_grid, cv=3, scoring=f1_scorer)
grid_search.fit(X_train, y_train)

best_C = grid_search.best_params_['classifier__C']
best_f1_score = grid_search.best_score_

print(f"Best C value: {best_C}")
print(f"Best f1-score value: {round(best_f1_score, 2)}")

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

test_data['NEW'] = test_data['feature_7'] * test_data['feature_11']

X_train = train_data.drop(columns=['target'])
y_train = train_data['target']

X_test = test_data.drop(columns=['target'])
y_test = test_data['target']

numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

numeric_transformer = SimpleImputer(strategy='median')
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

f1_scorer = make_scorer(f1_score)

models = {
    'RandomForest': RandomForestClassifier(random_state=42),
    'GradientBoosting': GradientBoostingClassifier(random_state=42),
    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000, solver='saga')
}

param_grids = {
    'RandomForest': {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [None, 10, 20]
    },
    'GradientBoosting': {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__learning_rate': [0.01, 0.1, 0.2],
        'classifier__max_depth': [3, 5, 7]
    },
    'LogisticRegression': {
        'classifier__C': np.logspace(-3, 2, num=6)
    }
}

best_model = None
best_score = 0
best_f1_score = 0
best_params = {}

for model_name, model in models.items():
    print(f"Training {model_name}...")

    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', model)
    ])

    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=3, scoring=f1_scorer)
    grid_search.fit(X_train, y_train)

    y_pred = grid_search.predict(X_test)
    test_f1_score = f1_score(y_test, y_pred)

    if test_f1_score > best_f1_score:
        best_f1_score = test_f1_score
        best_score = grid_search.best_score_
        best_model = model_name
        best_params = grid_search.best_params_

print("\n")
print(f"Best model: {best_model}")
print(f"Test F1-Score: {best_f1_score}")
print(f"Best score: {best_score}")
print(f"Best parameters: {best_params}")